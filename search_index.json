[
["index.html", "Russian influence on Iźva Komi sibilant articulation Section 1 Preface 1.1 Abstract 1.2 About using GitHub pages", " Russian influence on Iźva Komi sibilant articulation Niko Partanen 2016-05-24 Section 1 Preface I’m presenting this study at SLE 2016 conference in Naples. I’ve been working with it quite a while, and it connects in many ways to my PhD studies. Actually this page contains the whole study as it is. At the moment I have the data locally, but in the later phase also this will be connected to a repository where it can be accessed, at least after request in case some materials cannot be openly shared. 1.1 Abstract The Uralic language Komi Zyrian distinguishes several palatal consonants as their own phonemes. This differs from the palatalized consonants found in Russian. In some varieties of the Iźva dialect, the sibilants /ɕ/ and /ʑ/ have shifted to a palatalized manner of articulation. This feature is often characteristic of the Komi spoken in Western Siberia and on the Kanin and Kola Peninsulas. It is also clear that other authors have perceived this difference, i.e. by transcribing with /ś/ and /š́/ what presumably is the difference between /sʲ/ and /ɕ/ (cf. Iźva varieties in (Uotila 1986) with many different sibilant allophones). Native speakers also occasionally comment on this feature. This change is likely, though not necessarily, induced by Russian contact. As a change within allophonic variants, this change has had no impact on the Komi phonological system. To make a comparison, this already makes the contact outcome different from that of other Uralic language, Kildin Saami, where Russian influence has slightly changed the phonotaxis of palatalized consonants, among other things, as analyzed by Blokland et al. (2011, 16) and Kuzmenko et al. (2012). I have examined this phenomenon in connection with the adaptation of Russian loanwords in spoken Komi. The latter topic has generally received some attention, i.e. (J. A. Igušev 1972); for an overview of the language contact situation, see (Leinonen 2009). Due to bilingualism, Komi speakers always have the option of pronouncing the sibilants in lexical items such as ‘September’ following the Russian or Komi model, respectively /sʲenʲtʲabrʲ/ or /ɕeɲcabr/, with variation in levels of adaptation. When analyzed in a modern sociolinguistic framework where variables are seen as expressions of styles and social identities, not as permanent attributes of the speakers, e.g. (Eckert 2012, 93–94), this variation can be described within the context where it occurs. Among the relevant factors, formality seems to be salient: speech in an educational setting, official speeches or interviews exhibit more standard Komi pronunciation than informal conversations. The social relevance of this variation is connected to the perception that the palatalized pronunciation is not native to Komi. However, while the palatalized pronunciation can be perceived as a Russian accent, an overly Komi pronunciation of Russian lexical items can also be interpreted as an inability to pronounce Russian – something that seems to be poorly accepted in modern Russia. One must also note that for some speakers the palatalized pronunciation can be considered the unmarked variant. The analysis is based on a transcribed corpus of spoken Iźva Komi collected as part of a research project funded by Kone Foundation between 2014 and 2016. This larger body of data has been sampled so that there are examples from different age groups and different areas where Iźva Komi is spoken. Tools of phonetic analysis are used to measure the different variables and the distribution and relevance of the results are validated with statistical tests. In 2016, the material will be stored and archived in different repositories with the rest of the Komi corpus, making them available for scientific use and ensuring the reproducibility of the research. 1.2 About using GitHub pages One of the key reasons to manage the project this way publicly in GitHub is that it really ensures the reproducibility of the study. It isn’t necessarily so important for the others, but it is also easier for me to manage what is going on while the project is rigorously under version control. And so is the language documentation corpus the study is based upon. It is natural that in many cases the researchers cannot share all of their raw data, the main reasons being connected to the rights of those individuals from who the data has been collected. This is of course normal in human sciences. Sharing the research in prose form is also a norm, but as the we datasets used are getting more complicated, there is also more use of computational tools. Sharing these tools should be as natural as sharing the more literal content of your work. This is one way to do it nicely, in my opinion. 6 References "],
["introduction.html", "Section 2 Introduction 2.1 Transcription system 2.2 Phenomena 2.3 Research questions 2.4 Data", " Section 2 Introduction --> The Uralic language Komi Zyrian distinguishes several palatal consonants as their own phonemes. This differs from the palatalized consonants found in Russian. In some varieties of the Iźva dialect, the sibilants /ɕ/ and /ʑ/ have shifted to a palatalized manner of articulation. This feature is often characteristic of the Komi spoken in Western Siberia and on the Kanin and Kola Peninsulas. It is also clear that other authors have perceived this difference, i.e. by transcribing with /ś/ and /š́/ what presumably is the difference between /sʲ/ and /ɕ/ (cf. Iźva varieties in (Uotila 1986) with many different sibilant allophones). This change is likely, though not necessarily, induced by Russian contact. As a change within allophonic variants, this change has had no impact on the Komi phonological system. This already makes the contact outcome different from that of Kildin Saami, where Russian influence has slightly changed the phonotaxis of palatalized consonants, among other things, as analyzed by Blokland et al. (2011, 16) and Kuzmenko et al. (2012). I have examined this phenomenon in connection with the adaptation of Russian loanwords in spoken Komi. The latter topic has received some attention, i.e. (J. A. Igušev 1972); for an overview of the language contact situation, see (Leinonen 2009). Due to bilingualism, Komi speakers always have the option of pronouncing the sibilants in lexical items such as ‘September’ following the Russian or Komi model, respectively /sʲenʲtʲabrʲ/ or /ɕeɲcabr/, with much variation in levels of adaptation. When analyzed in a modern sociolinguistic framework where variables are seen as expressions of styles and social identities, not as permanent attributes of the speakers, e.g. (Eckert 2012, 93–94), this variation can be described within the context where it occurs. Among the relevant factors, formality seems to be salient: speech in an educational setting, official speeches or interviews exhibit more standard Komi pronunciation than informal conversations. The social relevance of this variation is connected to the perception that the palatalized pronunciation is not native to Komi. However, while the palatalized pronunciation can be perceived as a Russian accent, an overly Komi pronunciation of Russian lexical items can also be interpreted as an inability to pronounce Russian – something that seems to be poorly accepted in modern Russia. One must also note that for some speakers the palatalized pronunciation can be considered the unmarked variant. The analysis is based on a transcribed corpus of spoken Iźva Komi collected as part of a research project funded by Kone Foundation between 2014 and 2016. This larger body of data has been sampled so that there are examples from different age groups and different areas where Iźva Komi is spoken. Tools of phonetic analysis are used to measure the different variables and the distribution and relevance of the results are validated with statistical tests. In 2016, the material will be stored and archived in different repositories with the rest of the Komi corpus, making them available for scientific use and ensuring the reproducibility of the research. 2.1 Transcription system Uralic Phonetic Alphabet (henceforth UPA) is commonly used transcription system in Uralistics. It is very exact and in all real applications interchangeable with IPA. The main issue I see with UPA is that it is used in widely different manner by different researchers in the descriptions of different languages. This creates rather good uniformity within language varieties, i.e. transcribed Komi follows more or less the same conventions, as does transcribed Meadow Mari or Tundra Nenets, but these transcriptions are not easily comparable. And although being very exact in many nuances, it is well known that UPA does not distinguish palatal place of articulation from palatalized manner of articulation. Thereby following grapheme when used in transcription: ś Could represent either phones sʲ or ɕ There are very few languages in which these variants would be phonemically distinct. Naturally one can always come up with ad-hoc conventions to mark these differences, but this again feeds to the problem I described above: lack of comparability between transcripts. 2.1.1 Transcription system used in this study The transcription system used in Iźva Komi Documentation Project is in Cyrillic and it follows closely the conventions of Komi orthography. It is the same transcription system used in new Komi dialect dictionaries published recently (Безносикова Л. М. AND Айбабина 2012). It doesn’t mark allophonic variatian, but captures Komi phoneme inventory in a very satisfactory manner. As every pohonene is represented, it is possible to convert this transcription into other transcription systems, such as IPA or UPA. In this study I have used IPA as described in the International Phonetic Association’s handbook (International Phonetic Association 1999). The Cyrillic transcription used in IKDP is essentially phonemic, but due to conventions of Cyrillic orthography there is no one-to-one correspondence between the characters used. Instead, there are complex one-to-many correspondences, which nevertheless are systematic and resolving between these should, in principle, be possible in all cases. There are some corner cases and murky areas in Komi phonology, but in the larger picture those do not problematize the situation too much. There are several good reasons to select Cyrillic based orthography in the first place: Orthography represents already Komi phoneme inventory very well There is already good amount of language technology around Komi Zyrian written language, using Cyrillic transcription makes that compatible Native speakers have more immediate access to the data Cyrillic transcription is relatively ignorant about the realizations of underlying allophones, so one can conveniently remain at the phoneme level without putting spoon into too many soups 2.2 Phenomena Russian and Komi have relatively similar phonological distinction between plain velars and their “soft” variants. Orthographically they are marked similarly in both languages, using same conventions of Cyrillic orhography. The actual realizations of the phones, the allophones, within these phonemes are often rather distinct, Russian phoneme usually being rather prototypical alveolar palatalized sibilant /sʲ/. In Komi the phoneme in question is relatively different /ɕ/. Russian also contains phone [ɕ:], phonemic status of which seems to be debated (Chew 2003, 64), marked with Cyrillic grapheme &lt;щ&gt;. This phone seems to be very close to the phoneme /ɕ/ found in Komi. [ˈɕːæsʲtʲjə] --> 2.3 Research questions Is the variation connected to language contact If so, is Russian the source language How this variation can be described What is the areal scope of this variation What is the temporal scope of this variation Is the variant sociolinguistically significant If some of these questions cannot be answered, the reasons to this would also be worth its own investigation. 2.4 Data All my data comes from Iźva Komi Documentation Project which has been funded by Kone Foundation in 2014-2016 and led by Rogier Blokland, Michael Rießler and Marina Fedina. For the first two years of the project I worked in it as a full time PhD student, and my tasks were closely connected to curating, extending and annotating the Komi corpus in question. The project has collected large amount of new data from the areas where Iźva Komi is spoken, but at the same time it is closely connected to older data sources. Naturally, there is no thinking that we would be the first researchers collecting data from these speech varieties – already M.A. Castrén wrote his grammar based to this variery (Castren 1844). Although the majority of our effort has gone to working with our own data, we have also incorporated to the corpus data from variety of older sources. We have prioritized the data for which audio recordings exist, but even the materials with no audio can be extremely valuable. Naturally with mere transcriptions there arises the issue of verification – it is not possible to examine more closely what lies behind the transcribed segments. On the other hand, I think we must start our work with the expectation that the scholars who have worked before us must have had similarly high standards as we do. In some sense also quantity can be used to verify the quality, as the transcriptions which show systematic patterns in larger amounts of text are very likely valid. More so if these patterns are somehow connected to what can be observed in the current and contemporary recordings. The availability of earlier materials makes contemporary language corpora very heterogenic. Or better to say, they have the potential to do this. Many corpora are still created on basis of individual recording sessions in a fairly traditional settings, but as the world is full of online resources and printed materials, one is often tempted to include these into corpus whenever possible. This makes corpus curation more complicated task, and it demands that the corpus user has some access and understanding of the overall structure of the corpus. In real world applications the data selected for research use would often be a sample from the complete corpus. This sounds in many ways like a normal scientific procedure, so I think one can assume that’s a good way to go. In my case the most important subcorpora have been the Nenets Autonomous Okrug section of Iźva Komi Documentation Project, transcriptions provided in Syrjänische Texte II by T.E. Uotila, the recordings of Eric Vászolyi from Kanin Peninsula and Kolva region, and the TV broadcasts of Radio Yamala in early winter of 2016. 2.4.1 Open issues with data Lack of data from Russian dialects What are the limitations of language documentation corpora Can TV broadcast data be used to phonetic analysis? How to differentiate Russian loanwords from already native elements (for example, very old opaque loanwords) 6 References "],
["methodology.html", "Section 3 Methodology 3.1 Reproducibility 3.2 Archiving 3.3 ELAN annotations 3.4 Automated processing with R 3.5 Praat 3.6 Statistical analysis", " Section 3 Methodology Methodology always has two aspects – scientific and technical. The first is about what do we want to do, and the latter deals with how do we do it. In principle the ideal technical framework would allow us to focus only to the scientific side, as that is where the linguistic questions are being formulated and solved. However, in practice we almost inevitably have to settle to something that is not perfect, but allows us to get the work done. I’m personally quite strict with this, and having some programming background, I’m very reluctant to get into manual labour which I see computer could perform better, faster and more reliably. However, I think it is very important to identify what are those parts where manual labour is needed and valuable, and built the workflow so that the possible manual strain would be on exactly those sections where human input matters. There are many workflows surrounding sociolinguistic data, for example, one suggested by Nagy and Meierhof (Nagy and Meyerhoff 2015). I’m suggesting something very much in that strain, but with the difference that this model is, as I see it, more reliable and easy to maintain. The workflow they describe is still very much centered with exporting files, whereas I would see this as most of the time unnecessary step, since the data we want to deal with is already very well presented in the files we have in the beginning. Parsing this data directly to our analysis has the benefit that the changes in data is immediately represented in the further results. In many ways it is often possible to imagine ideal workflow. However, as important as this is, one also has to realize the limitations which may make this unrealistic. A very common issue is that the current tools we have do not allow fast enough annotation if the annotations have to be fit into a very byzantine and complex data structures. Also the problem tends to arise that even though creating annotations now would work, there can be some problems which occur later when the annotations are edited, realigned and corrected. It is possible to delineate following principles: One piece of information should be written only once Data should be interconnected in the way that relations between different units can be reconstructed (this doesn’t need to be simple or straightforward, but these relations have to exist one way or another in machine readable format. More the connections are documented, the better.) Working with annotations should be relatively rapid and effortless In reality one has to balance between these principles. Of course one could argue that the last one is easiest to cut from, but the case is not so straightforward, as it has direct impact to the working hours spent which eventually comes into the number of occurrences in the corpus. This is especially important if more statistical methods are employed, for which it may be reasonable to have several parallel datasets used for testing different hypotheses. If data editing is extremely tedious, then it becomes much less feasible that the data is treated as would be the most suitable. And all this kind of boils down to the fact: There has to be a long-term data curation plan where these study-specific annotations fit Even though some annotations would be done just for one study, there is no reason they should be separated from the rest of the corpus. However, they should be added to the corpus in a way which doesn’t demand extra work from normal data curation. This means that they should not be too fixed to the current values, for example, they should not assume that elements such as tokens are immutable. 3.1 Reproducibility By saying that my study is reproduceable I do not claim that my results are necessarily correct. I only mean that the others can verify what I have done and evaluate it with the data I have used. Ideally the study results would be repeated with a different dataset, which would in many ways be actually desired, since that would immediately add credibility to my ideas presented here. As reproducibility is a rather new concept in linguistic sciences, there seems to be quite a bit of confusion over the terms reproduction, replication and independent verification. Biostatistician Roger Peng has written very clearly about this in Simply Statistics blog (Peng 2014). The study follows logic not particularly unusual for a scientific study: a phenomena has been observed and found interesting after further contextualization. A further study was planned in order to find out what it is about, so a corpus sample has been selected and annotated. These annotations are analyzed and the results are interpreted against the ideas that arose after the initial observation and contextualisation. The analysis part is mainly within the 4 Conclusions section of this study. 3.2 Archiving The corpus files are archived in The Language Archive of Nijmegen Max Planck institute. They are organized under the node Permic_Varieties/kpv/. In the archive structure all sessions are treated as entities on same hierarchy level. This may seem confusing, but the idea is that one has to consult metadata in order to know into which project and working group which session is connected. It would had been customary to organize data by academic projects. However, this model becomes problematic when the same data has been collected, transcribed, digitalized, reanalyzed and archived by totally disconnected people often many decades apart from one another. Thereby the only model to arrange these files in an honest manner is to list in metadata all relevant actors and projects involved. In the archive there has been an attempt to maintain rough dialect division, so that the files would be named by prefixes marking different varieties. This is also complicated topic, as same recording contains often data from speakers of different dialects. Individual speakers have often been very mobile, so they also can have mixed features in their everyday speech. As many other good ideas, also this dialect based file naming has been somewhat futile. The subcorpus used in this study today, 2016-05-24, may not be connected to those files the archive contains in whatever future date this study is being read. However, this study refers explicitly to those file versions which were valid and accessible at the time when this document was compiled. 3.3 ELAN annotations The ELAN annotation model used in the corpus is relatively simple, the data being set on different layers which correspond to specific time-aligned utterances. This means that for one Komi utterance there are often different data layers, for example, Russian translation, English translation or comments. Also tokenized data layer is provided, but the tokenized units are not connected to the recording itself. For this study a new layer is added on which the tokens containing sibilants are time aligned and transcribed in IPA. This has been done by copying the token containing tier, filtering out the tokens which do not contain palatal sibilants and time aligning the remaining word forms. The steps taken have been: Copying tier of symbolic subdivision type which contains the tokens into tier of included in type Filtering the sibilant containt forms with regular expression ^((?!зь)(?!зё)(?!зя)(?!зю)(?!зи)(?!зе)(?!сь)(?!сё)(?!ся)(?!сю)(?!си)(?!се)(?!Зь)(?!Зё)(?!Зя)(?!Зю)(?!Зи)(?!Зе)(?!Сь)(?!Сё)(?!Ся)(?!Сю)(?!Си)(?!Се)(?!тс)(?!ст)(?!Ст).)*$, which selects everything that can’t contain a feature I am interested, and replacing it’s result with an empty string Using ELAN’s Remove annotations or values tool to remove all annotations on this tier when they contain an empty annotation With this workflow we get from this: Elan file in the original state To this, which can relatively easily be time aligned: Added sib-tier with only sibilants left The transcription system used is described in section 2.1. about transcription systems. This means that an utterance: пукав да сёй! Sit.down.IMP and eat.IMP Sit down and eat! Would be transcribed on the sibilant annotation tier as: ɕoj or sʲoj This is not a perfect method, but the best I was able to come up within the rationale and limitations explained above. Now there is still quite much manual work in translitteration to phonemic IPA, aligning everything and identifying the necessary phonetic adjustments. Thereby I wrote a small function which does the transliteration. The screenshot below illustrates the changes it does inside ELAN file: Sibilant tier before and after running sle2016partanen::sib_tier_cyr2ipa function After this each token on sibilant tier is manually aligned and evaluated. This is relatively time consuming phase, but in many ways it is also the most important one. The whole study is in the end about these variables, so spending most of the time with analysing them is indeed quite appropriate. What I’ve tried to reach in this workflow is state where minimal amount of time is spent in rather unimportant manual tasks, and thereby what really matters gets the time it deserves. In the end automated workflow also makes it possible to reach relatively large sample within the current time frame. Although time is not really the most pressing issue, our daily working hours are in the end always limited. 3.3.1 Caveouts of the current annotation model The problem with the current model is that it is pretty manual and the annotated sibilant containing forms are not strictly connected to the corresponding tokens within the utterances. The main reason to this is that I want the sibilant annotated tokens to remain independent from further changes in the main transcription. It is also somewhat problematic that the actual annotation is usually done in a longer time span and the exact method may be refined. For example, one may realize later a context which should had been taken into account in the regular expression. This leads to a change which is automatically implemented in the further files, but must be manually carried out in older files. This is not a huge problem in itself, but it means that the workflow is easily not as reproduceable and automatic as it may seem. 3.4 Automated processing with R In order to retrieve the sibilant information from ELAN files an R package FRelan is used. It contains several functions for parsing data from ELAN files to a data frame, which is an R internal tabular format which works rather well for data like this. Each token forms its own row. This data frame is merged with metadata which contains biographical and geographical information about, for example, about the speakers themselves and the time and place related information about the context where the utterance was recorded. The variables relevant for my analysis are: Age of the speaker Birthplace of the speaker Place of residence of the speaker Recording time Recording setting (formal, informal) Education of the speaker These variables are compared to the frequency of different sibilant allophones. 3.5 Praat Most of the time the distinction between these allophones is relatively easy to hear. However, I have used Praat in many cases to establish the baseline for these features, and also to validate whether some cases are as complex as they sound like. 3.6 Statistical analysis 6 References "],
["conclusions.html", "Section 4 Conclusions 4.1 Stable variation or change in progress 4.2 Variation of variable 4.3 Comparable case studies", " Section 4 Conclusions With many earlier studies it has been taken as granted that Iźva dialect of Komi Zyrian would have palatalized sibilants as a characteristic feature of this dialect, see for example (J. Igušev 1972), or their status is left uncovered but explicitly shows as same as in Standard Komi (Сахарова 1976). My study clearly challenges this view and proposes that in principle Iźva Komi has identical system of sibilants as there is in the Standard Komi, but some subvarieties of Iźva Komi differ from this. It is important to notice, that in the wider ethno-historical context it is necessary to view the palatalal sibilants as the primary and earlier feature of Iźva Komi, and the forms found in different subdialects as a later innovation. The source of palatalized manner of articulation is most likely contact with Russian. However, placing these developments into their complete contexts is a more difficult undertaking. It has been described in earlier studies how the Russian dialects in Ust’-Tsilma area have been in mutual influence with Iźva Komi for several centuries, and this has to be taken into account somehow, especially when palatalization in itself seems to be typical for those Iźva varieties spoken way outside this contact sphere. Since it occurs in both extremes of Iźva territory: Kola Peninsula and Siberia, one must conclude that the most likely point of origin is somewhere in Mala- and Bol’šemel’skaya Tundra. 4.1 Stable variation or change in progress 4.2 Variation of variable 4.3 Comparable case studies This section is just for personal notes. One should collect a good list of similar case studies so that it would be easier to imitate their methodology and style. 4.3.1 Final sibilant deletion in Brazilian Portuguese This is reported i.e. in (Bayley and Lucas 2007, 8). 4.3.2 Salvadoran Spanish -s deletion and aspiration Some Hoffman has been studying this quite a bit. 6 References "],
["technical-documentation.html", "Section 5 Technical documentation 5.1 Code 5.2 Conversion patterns 5.3 Functions 5.4 Manual tasks", " Section 5 Technical documentation This part is the technical documentation of programming part behind this study. In some sense the code should be self explanatory as it is well commented, but it is understandable that it is not very clear for those who are not familiar with the programming language used – R. I have tried to explain some very basic concepts so that also non-technical readers can familiarize themselves with this section, but the point of that is to explain what the code does, what is the human logic behind it and what are the tasks it does. The idea is not to make the code itself understandable for everyone, as this probably would not be possible. However, explaining the logic behind it must be possible. 5.1 Code I had initially named this section as Scripts, however, it doesn’t really describe well R based workflows. A script usually refers to a chunk of code which is executed relatively statically. It is ran, and something happens. However, scripts can often be given arguments, for example, one could specify input and output and give some additional parameters for the script. A function doesn’t differ particularly much from this, but it is more flexible unit that can be called from within different code chunks. So instead of taking x and producing y, a function can be a part of workflow where different function are interacting with one another without saving the intermediate files inbetween. Only actual script used here is compile.R file which has to be executed in order to compile this document. In my workflow used in this study the following tasks have been assigned to different R functions. CYR to IPA conversion FRelan R package for daily tasks of loading files Extracting sibilant values Matching lexical items Reading metadata Counting occurrences in the context Visualizations Statistical analysis 5.2 Conversion patterns The automatic conversion from Cyrillic transcription to IPA plays rather important part in this study, so how exactly this conversion is done is important to document. The patterns are stored in ikdp2ipa.csv file, and they can be freely examined or reused. 5.3 Functions 5.3.1 sle2016partanen::transliterate The transliteration pattern has currently some shortcomings, but they are relatively minimal and as every item as to be gone through manually anyway, this is not really an issue. For example, it is not always transparent whether consecutive sibilants such as ссь represent combination ɕɕ or sɕ. Naturally, there is often some kind of assimilation going on there anyway, although trying to identify the pattern in this could be useful task in itself later. What is written above is mainly about the pattern involved, the idea being that transliterate function can take basically any CSV-formatted file from where it read the patters. Why CSV and not something fancier? Because editing CSV is EASY. sle2016partanen::transliterate ## function(data, model){ ## pattern &lt;- read.csv(file = model, sep = &quot;,&quot;) ## cyr &lt;- as.character(pattern[,1]) ## lat &lt;- as.character(pattern[,2]) ## multiple_replace(cyr, lat, data) ## } ## &lt;environment: namespace:sle2016partanen&gt; 5.3.2 sle2016partanen::sib_tier_cyr2ipa transliterate function is now hard-coded into this so that it runs ikdp2ipa conversion to the sib-tier. In principle this function could be generalized into something more abstract with the idea take the tier elements, do something for each item, put them back. sle2016partanen::sib_tier_cyr2ipa ## function(eaf_file = &quot;./R/kpv_izva20140404IgusevJA.eaf&quot;, participant = &quot;JAI-M-1939&quot;, linguistic_type = &quot;sib&quot;){ ## ## `%&gt;%` &lt;- dplyr::`%&gt;%` ## ## file &lt;- xml2::read_xml(eaf_file) ## ## dplyr::data_frame( ## Content = file %&gt;% ## xml2::xml_find_all( ## paste0(&quot;//TIER[@LINGUISTIC_TYPE_REF=&#39;&quot;, linguistic_type, &quot;&#39; and @PARTICIPANT=&#39;&quot;, ## participant,&quot;&#39;]/ANNOTATION/*/ANNOTATION_VALUE&quot;)) %&gt;% ## xml2::xml_text(), ## annot_id = file %&gt;% ## xml2::xml_find_all(paste0(&quot;//TIER[@LINGUISTIC_TYPE_REF=&#39;&quot;, linguistic_type, &quot;&#39; and @PARTICIPANT=&#39;&quot;, ## participant,&quot;&#39;]/ANNOTATION/*/ANNOTATION_VALUE/..&quot;)) %&gt;% ## xml2::xml_attr(&quot;ANNOTATION_ID&quot;), ## ref_id = file %&gt;% ## xml2::xml_find_all( ## paste0(&quot;//TIER[@LINGUISTIC_TYPE_REF=&#39;&quot;, linguistic_type, &quot;&#39; and @PARTICIPANT=&#39;&quot;, ## participant,&quot;&#39;]/ANNOTATION/*/ANNOTATION_VALUE/..&quot;)) %&gt;% ## xml2::xml_attr(&quot;ANNOTATION_REF&quot;), ## ts1 = file %&gt;% ## xml2::xml_find_all( ## paste0(&quot;//TIER[@LINGUISTIC_TYPE_REF=&#39;&quot;, linguistic_type, &quot;&#39; and @PARTICIPANT=&#39;&quot;, ## participant,&quot;&#39;]/ANNOTATION/*/ANNOTATION_VALUE/..&quot;)) %&gt;% ## xml2::xml_attr(&quot;TIME_SLOT_REF1&quot;), ## ts2 = file %&gt;% ## xml2::xml_find_all( ## paste0(&quot;//TIER[@LINGUISTIC_TYPE_REF=&#39;&quot;, linguistic_type, &quot;&#39; and @PARTICIPANT=&#39;&quot;, ## participant,&quot;&#39;]/ANNOTATION/*/ANNOTATION_VALUE/..&quot;)) %&gt;% ## xml2::xml_attr(&quot;TIME_SLOT_REF2&quot;), ## participant = file %&gt;% ## xml2::xml_find_all( ## paste0(&quot;//TIER[@LINGUISTIC_TYPE_REF=&#39;&quot;, linguistic_type, &quot;&#39; and @PARTICIPANT=&#39;&quot;, ## participant,&quot;&#39;]/ANNOTATION/*/ANNOTATION_VALUE/../../..&quot;)) %&gt;% ## xml2::xml_attr(&quot;PARTICIPANT&quot;), ## tier_id = file %&gt;% ## xml2::xml_find_all( ## paste0(&quot;//TIER[@LINGUISTIC_TYPE_REF=&#39;&quot;, linguistic_type, &quot;&#39; and @PARTICIPANT=&#39;&quot;, ## participant,&quot;&#39;]/ANNOTATION/*/ANNOTATION_VALUE/../../..&quot;)) %&gt;% ## xml2::xml_attr(&quot;TIER_ID&quot;), ## type = file %&gt;% ## xml2::xml_find_all( ## paste0(&quot;//TIER[@LINGUISTIC_TYPE_REF=&#39;&quot;, linguistic_type, &quot;&#39; and @PARTICIPANT=&#39;&quot;, ## participant,&quot;&#39;]/ANNOTATION/*/ANNOTATION_VALUE/../../..&quot;)) %&gt;% ## xml2::xml_attr(&quot;LINGUISTIC_TYPE_REF&quot;)) -&gt; content ## ## # In the data frame content there is now the original content of the tier ## # We replace this with transliterated variant ## ## content %&gt;% dplyr::mutate(Content = sle2016partanen::transliterate(tolower(Content), &quot;R/ikdp2ipa.csv&quot;)) -&gt; content ## ## tier &lt;- XML::newXMLNode(&quot;TIER&quot;, attrs = c(DEFAULT_LOCALE = &quot;en&quot;, ## LINGUISTIC_TYPE_REF = linguistic_type, ## PARENT_REF = paste0(&quot;ref@&quot;, participant), ## # LANG_REF = lang, ## PARTICIPANT = participant, ## TIER_ID = paste0(&quot;sib@&quot;, participant))) ## ## # This adds annotation element under tier, so it already looks like this: ## # ## # &lt;TIER LINGUISTIC_TYPE_REF=&quot;sib&quot; PARENT_REF=&quot;ref@JAI-M-1939&quot; PARTICIPANT=&quot;JAI-M-1939&quot; TIER_ID=&quot;sib@JAI-M-1939&quot;&gt; ## # &lt;ANNOTATION/&gt; ## # &lt;/TIER&gt; ## ## # Next step is to populate that tier ## ## plyr::d_ply(content, .variables = &quot;annot_id&quot;, function(x){ ## annotation &lt;- XML::newXMLNode(&quot;ANNOTATION&quot;, parent = tier) ## alignable_annotation &lt;- XML::newXMLNode(&quot;ALIGNABLE_ANNOTATION&quot;, ## attrs = c(ANNOTATION_ID = x$annot_id, ## TIME_SLOT_REF1 = x$ts1, ## TIME_SLOT_REF2 = x$ts2), ## parent = annotation) ## annotation_value &lt;- XML::newXMLNode(&quot;ANNOTATION_VALUE&quot;, x$Content, parent = alignable_annotation) ## alignable_annotation ## }) ## ## ## doc &lt;- XML::xmlParse(eaf_file) ## ## # Note: add speaker attribute so that we don&#39;t delete too many tiers! ## ## XML::removeNodes(doc[paste0(&quot;//TIER[@LINGUISTIC_TYPE_REF=&#39;&quot;, linguistic_type, &quot;&#39;]&quot;)]) ## ## # doc ## eaf_to_be_written &lt;- XML::getNodeSet(doc, &quot;//ANNOTATION_DOCUMENT&quot;) ## ## XML::xmlChildren(eaf_to_be_written[[1]]) &lt;- XML::addChildren(eaf_to_be_written[[1]], tier) ## ## XML::xmlChildren(eaf_to_be_written[[1]]) &lt;- c(XML::xmlChildren(eaf_to_be_written[[1]]))[c(order(factor(names(eaf_to_be_written[[1]]), ## levels = c(&quot;HEADER&quot;, ## &quot;TIME_ORDER&quot;, ## &quot;TIER&quot;, ## &quot;LINGUISTIC_TYPE&quot;, ## &quot;LOCALE&quot;, ## &quot;LANGUAGE&quot;, ## &quot;CONSTRAINT&quot;, ## &quot;CONTROLLED_VOCABULARY&quot;, ## &quot;EXTERNAL_REF&quot;))))] ## ## new_filename &lt;- gsub(&quot;(.+)(.eaf)$&quot;, &quot;\\\\1-ipa.eaf&quot;, eaf_file) ## ## XML::saveXML(eaf_to_be_written[[1]], new_filename) ## ## } ## &lt;environment: namespace:sle2016partanen&gt; 5.4 Manual tasks Basic annotations on ELAN files are done manually. Similarly the work relies to many features of ELAN, for example, tokenization and copying tiers is done with built-in tools. "],
["references.html", "Section 6 References", " Section 6 References "]
]
